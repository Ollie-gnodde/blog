---
title: Why Data Scientists aren't Doctors!
author: ~
date: '2017-07-17'
slug: second-new-post-in-r-markdown
categories: []
tags: []
---



<div id="intro" class="section level1">
<h1>Intro</h1>
<p>Technology is advancing at an unbelievable rate, and one of the most exciting, not to mention important, fields that is benefitting from this is the field of Medicine. From CAT scans to cancer cures, science and technology is making the impossible possible with treatments and procedures so advanced and effective, they almost border on the mythical. There are however some drawbacks. One of the most common ones that is joked around the proverbial water-cooler is self diagnosis using technology. In this day and age, if you tell someone that you’ve got a sore throat and feel a little under the weather, you’ll most likely be told “Don’t Google your symptoms, it’ll probably say you’re pregnant!”</p>
<p>As a prospective data scientist, I find this situation worrying, if not highly offensive… How is it possible that we aren’t able to accurately diagnose ourselves using the same advanced techniques that can effectively model the stock market, crime rates and pokemon cards?! And so, when the opportunity arose to find a data set, build a functional model and then use it to analyse said data set came up, I took it with both hands.</p>
<p>Full disclosure… I have included the entire code (except for the part about the packages needed), so if you are a BA major, you may want to look away now; if not, you should find it helpful should you want to do something similar.</p>
</div>
<div id="code-models-and-some-explanation" class="section level1">
<h1>Code, models and some explanation</h1>
<p>The first thing I had to do was to find some usable data. I settled on a set of just over 300 diagnoses of heart conditions, along with 13 possible factors sucg as age, max heart rate achieved under exercise and an ecg reading. These were factors that I expected to be highly correlated with heart conditions, and so expected a good model. If you are curious, you can find the data <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data">here</a>. The data is all numeric, so the heart diseases are ranked on a scale from 0 to 4, where 0 is completely healthy and 4 is an impending heart-attack.</p>
<p>If you check out that link, you will see that it just leads to a bunch of numbers on a screen. I therefore had to scrape the website and then format the data (a decent amount of work given my relative inexperience in the field), which was done with the code below. Of course, before doing even that, we have to import the necessary libraries, which are</p>
<p>library(rvest) library(stringr) library(dplyr) library(rpart) library(caret) library(vcdExtra) library(ipred) library(neuralnet) library(magrittr)</p>
<pre class="r"><code>#------------------------------Getting and processing our data----------------------------------------------
# Scrapes the data
raw_data &lt;- read_html(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data&quot;) %&gt;%  #reads the the data from this page
  html_nodes(&quot;body&quot;) %&gt;%     
  html_text()

# Turns the data into a list 
no_n &lt;- gsub(&quot;\n&quot;, &quot; ,&quot;, raw_data)
row_data &lt;- strsplit(no_n, &quot;,&quot;)


data &lt;- data.frame(matrix(unlist(row_data), nrow=303, byrow=T))

colnames(data)[1] &lt;- &quot;Age&quot;
colnames(data)[2] &lt;- &quot;Sex&quot;
colnames(data)[3] &lt;- &quot;Chest_Pain&quot;
colnames(data)[4] &lt;- &quot;Blood_Pressure&quot;
colnames(data)[5] &lt;- &quot;Cholestoral&quot;
colnames(data)[6] &lt;- &quot;Blood_Sugar&quot;
colnames(data)[7] &lt;- &quot;ecg&quot;
colnames(data)[8] &lt;- &quot;Max_Heart_Rate&quot;
colnames(data)[9] &lt;- &quot;eia&quot;
colnames(data)[10] &lt;- &quot;Exercise_induced_feelings&quot;
colnames(data)[11] &lt;- &quot;Slope_of_ST&quot;
colnames(data)[12] &lt;- &quot;Fluoroscopy_stain&quot;
colnames(data)[13] &lt;- &quot;Thal&quot;
colnames(data)[14] &lt;- &quot;Diagnosis&quot;

#taking away all non useful cells
data[ data == &quot;?&quot; ] &lt;- NA
data &lt;- na.omit(data)


head(data)</code></pre>
<pre><code>##    Age Sex Chest_Pain Blood_Pressure Cholestoral Blood_Sugar ecg
## 1 63.0 1.0        1.0          145.0       233.0         1.0 2.0
## 2 67.0 1.0        4.0          160.0       286.0         0.0 2.0
## 3 67.0 1.0        4.0          120.0       229.0         0.0 2.0
## 4 37.0 1.0        3.0          130.0       250.0         0.0 0.0
## 5 41.0 0.0        2.0          130.0       204.0         0.0 2.0
## 6 56.0 1.0        2.0          120.0       236.0         0.0 0.0
##   Max_Heart_Rate eia Exercise_induced_feelings Slope_of_ST
## 1          150.0 0.0                       2.3         3.0
## 2          108.0 1.0                       1.5         2.0
## 3          129.0 1.0                       2.6         2.0
## 4          187.0 0.0                       3.5         3.0
## 5          172.0 0.0                       1.4         1.0
## 6          178.0 0.0                       0.8         1.0
##   Fluoroscopy_stain Thal Diagnosis
## 1               0.0  6.0        0 
## 2               3.0  3.0        2 
## 3               2.0  7.0        1 
## 4               0.0  3.0        0 
## 5               0.0  3.0        0 
## 6               0.0  3.0        0</code></pre>
<p>As you can see, the data comes out nicely in a data frame. Note that I have taken away all cells that contained “?” (there were a couple) as these would have messed up my models later.</p>
<p>The first thing I decided to was to model it using a decision tree, as this is a fairly simple model, but one that deals well with a large number of factors. To do this, we make use of the Caret package. This not only makes implementation easy, but also bootstraps the data 25 times and gives us a very nice output off results.</p>
<p>In order to implement the model, we first divide our data in testing and training parts. I have opted for a 75/25 split, but it doesn’t really matter as long as you have a split and the training set is in the significant majority. We do this so that we can realistically test our model. If you don’t see why this is necessary, think of it like this… imagine trying to teach a child basic maths. You want to teach him addition, so you tell him 2 + 2 = 4, 2 + 3 = 5 and so on. If you then go back and test him by asking him 2 +2, he should (hopefully) say the answer is 4. However, you can’t be sure if he has learnt addition, or if he just knows that 2 + 2 = 4. In order to test him, you ask a sum he hasn’t seen before, like 2 + 4. If he answers 6, you’re doing a good job… anything else, and not so much.</p>
<p>Back to the code. We then train our model with Diagnosis as a finction of everything (thats what ~ . means) and instruct it to use the rpart method, which is a decision tree. The next two lines just show us the details and decision tree for the model.</p>
<pre class="r"><code>#----------------------------------Training our model---------------------------------------------------

# Splitting the data into training and testing sets

index &lt;- sample(1:nrow(data),round(0.75*nrow(data)))

train &lt;- data[index,]
test &lt;- data[-index,]

# We then train our model using the training data set 
data.rpart &lt;- train(Diagnosis ~ ., data = train, method = &quot;rpart&quot;)
# We can look at the details of our model here
data.rpart</code></pre>
<pre><code>## CART 
## 
## 223 samples
##  13 predictor
##   5 classes: &#39;0 &#39;, &#39;1 &#39;, &#39;2 &#39;, &#39;3 &#39;, &#39;4 &#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 223, 223, 223, 223, 223, 223, ... 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa    
##   0.01904762  0.5502062  0.2735190
##   0.03809524  0.5580310  0.2738411
##   0.06666667  0.5521312  0.2176496
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.03809524.</code></pre>
<pre class="r"><code># We can look at the decision tree here
data.rpart$finalModel</code></pre>
<pre><code>## n= 223 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 223 105 0  (0.53 0.18 0.12 0.12 0.054)  
##   2) Thal3.0&gt;=0.5 123  27 0  (0.78 0.14 0.041 0.033 0.0081) *
##   3) Thal3.0&lt; 0.5 100  77 1  (0.22 0.23 0.22 0.22 0.11)  
##     6) Chest_Pain4.0&lt; 0.5 32  16 0  (0.5 0.22 0.16 0.062 0.062) *
##     7) Chest_Pain4.0&gt;=0.5 68  48 3  (0.088 0.24 0.25 0.29 0.13) *</code></pre>
<pre class="r"><code>#--------------------------Testing the rpart model and looking at output---------------------------------

predict(data.rpart, test)</code></pre>
<pre><code>##  [1] 0  3  0  0  0  0  0  3  0  0  0  0  3  0  0  3  3  0  0  0  0  3  0 
## [24] 0  0  0  0  3  0  3  3  0  0  0  0  0  0  0  3  0  0  3  0  3  3  3 
## [47] 0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  3  0  0  0  0  3  3  0 
## [70] 0  0  3  3  3 
## Levels: 0  1  2  3  4</code></pre>
<pre class="r"><code>confusionMatrix(predict(data.rpart, test), test$Diagnosis)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction 0  1  2  3  4 
##         0  38  7  2  5  1
##         1   0  0  0  0  0
##         2   0  0  0  0  0
##         3   4  7  6  4  0
##         4   0  0  0  0  0
## 
## Overall Statistics
##                                           
##                Accuracy : 0.5676          
##                  95% CI : (0.4472, 0.6823)
##     No Information Rate : 0.5676          
##     P-Value [Acc &gt; NIR] : 0.5487          
##                                           
##                   Kappa : 0.2264          
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: 0  Class: 1  Class: 2  Class: 3  Class: 4 
## Sensitivity             0.9048    0.0000    0.0000   0.44444   0.00000
## Specificity             0.5312    1.0000    1.0000   0.73846   1.00000
## Pos Pred Value          0.7170       NaN       NaN   0.19048       NaN
## Neg Pred Value          0.8095    0.8108    0.8919   0.90566   0.98649
## Prevalence              0.5676    0.1892    0.1081   0.12162   0.01351
## Detection Rate          0.5135    0.0000    0.0000   0.05405   0.00000
## Detection Prevalence    0.7162    0.0000    0.0000   0.28378   0.00000
## Balanced Accuracy       0.7180    0.5000    0.5000   0.59145   0.50000</code></pre>
<p>Looking at the output of the code, we see that we have 223 samples for the training set, with 13 predictors and 5 classes. This all sounds good. One interesting aspect of the rpart model is that it is able to give us different complexity parameters and their subsequent accuracy is used in the model. The complexity parameter is effectively a measurement of how deep your decision tree is. If this doesn’t make sense, I suggest doing some background reading on it. We see which complexity parameter gives us the highest level of accuracy, which can be seen to be worryingly low here. Not to worry however, as we haven’t tested the model yet. We can also have a look at the decision tree if we wish to.</p>
<p>We then moving on to testing our model. We use the predict command, which predicts the Diagnosis of the 75 remaining data points using the 13 factors. We can see the output. This is nice to see, but really that useful. What is useful however, is the confusion matrix. This shows us how many false/true positive/negatives were produced by the model on the testing data set, as well as the overall statistics. We can see the sensitivity and specificity for each result, as well as the overall accuracy. Our accuracy is surprisingly low given that we expected the factors to be highly correlated to the output.</p>
<p>A low accuracy value is not the end of the world, for several reasons. The first of these is we have chosen an inappropriate model for the data set. Following this thinking, I tried a different tact and so I ran a logistical regession model. This has two benefits. The first is that it is another model and so we may find it is accurate. However, it is also a rather simple model and as the decision tree failed, I was not hopeful. The main reason I ran it is that this model is able to tell you which factors are strongly correlated, and which are not, to the output. We want to know this because sometimes having extraneous factors in the model makes it less accurate, and so if we are able to cut some unnecessary ones, it should help.</p>
<p>Once again, we divide the data into testing and training sets. A subtlety here is that the logistic model strictly takes numeric arguments, so we have to make sure the dataframe is a numeric one. Onc again we train the model as Diagnosis as a function of everything with the training data set and then we have a look.</p>
<pre class="r"><code>#-----------------------------Using a Logistics model--------------------------------------------------

#We first turn the data into a numeric dataframe and get new training and testing sets
data &lt;- as.data.frame(sapply(data, as.numeric))
index &lt;- sample(1:nrow(data),round(0.75*nrow(data)))
train &lt;- data[index,]
test &lt;- data[-index,]


logistic &lt;- glm(Diagnosis ~ . , data = train)
summary(logistic)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Diagnosis ~ ., data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3423  -0.5543  -0.0974   0.4858   2.8671  
## 
## Coefficients:
##                             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)               -2.011e+00  4.907e-01  -4.099 5.94e-05 ***
## Age                       -5.789e-05  7.465e-03  -0.008  0.99382    
## Sex                        1.396e-01  1.342e-01   1.040  0.29954    
## Chest_Pain                 2.123e-01  6.840e-02   3.104  0.00217 ** 
## Blood_Pressure             5.633e-03  5.539e-03   1.017  0.31031    
## Cholestoral               -3.079e-04  1.550e-03  -0.199  0.84273    
## Blood_Sugar               -1.136e-01  1.655e-01  -0.687  0.49298    
## ecg                        1.443e-01  6.037e-02   2.390  0.01773 *  
## Max_Heart_Rate            -3.133e-03  3.179e-03  -0.986  0.32548    
## eia                        3.768e-01  1.448e-01   2.603  0.00991 ** 
## Exercise_induced_feelings  2.039e-02  7.414e-03   2.750  0.00648 ** 
## Slope_of_ST                1.079e-01  1.158e-01   0.932  0.35236    
## Fluoroscopy_stain          4.516e-01  6.807e-02   6.635 2.73e-10 ***
## Thal                       3.021e-01  7.178e-02   4.209 3.81e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.7212286)
## 
##     Null deviance: 365.00  on 222  degrees of freedom
## Residual deviance: 150.74  on 209  degrees of freedom
## AIC: 575.51
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<pre class="r"><code>plot(logistic)</code></pre>
<p><img src="/post/Analysis_heart_conditions_files/figure-html/unnamed-chunk-4-1.png" width="672" /><img src="/post/Analysis_heart_conditions_files/figure-html/unnamed-chunk-4-2.png" width="672" /><img src="/post/Analysis_heart_conditions_files/figure-html/unnamed-chunk-4-3.png" width="672" /><img src="/post/Analysis_heart_conditions_files/figure-html/unnamed-chunk-4-4.png" width="672" /></p>
<p>The first thing that we see when we examine the sumamry is that not all of our factors are important. We can see it in two ways. The technical way is to see which factor has a small (on the order of 10^(-3)) p factor. The slightly easier way is to look at the stars next to the factors… *** means important, means not. I was happy to see that 8 of our factors seem to have little or no bearing on the model, and so we can take them out.</p>
<p>I was less happy with the plots. As you can see, the model struggled to successfully model the data points. I was not discouraged however, as I had one more trick up my sleeve… the Neural Network</p>
</div>
