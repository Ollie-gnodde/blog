---
title: Why Data Scientists aren't Doctors!
author: ~
date: '2017-07-17'
slug: analysis-of-heart-disease
categories: []
tags: []
---

Technology is advancing at an unbelievable rate, and one of the most exciting, not to mention important, fields that is benefitting from this is the field of Medicine. From CAT scans to cancer cures, science and technology is making the impossible possible with treatments and procedures so advanced and effective, they almost border on the mythical. There are however some drawbacks. One of the most common ones that is joked around the proverbial water-cooler is self diagnosis using technology. In this day and age, if you tell someone that you've got a sore throat and feel a little under the weather, you'll most likely be told "Don't Google your symptoms, it'll probably say you're pregnant!"     
.

As a prospective data scientist, I find this situation worrying, if not highly offensive... How is it possible that we aren't able to accurately diagnose ourselves using the same advanced techniques that can effectively model the stock market, crime rates and pokemon cards?! And so, when the opportunity arose to find a data set, build a functional model and then use it to analyse said data set came up, I took it with both hands.    
.

Full disclosure... I have included the entire code (except for the part about the packages needed), so if you are a BA major, you may want to look away now; if not, you should find it helpful should you want to do something similar.   
.


The first thing I had to do was to find some usable data. I settled on a set of just over 300 diagnoses of heart conditions, along with 13 possible factors sucg as age, max heart rate achieved under exercise and an ecg reading. These were factors that I expected to be highly correlated with heart conditions, and so expected a good model. If you are curious, you can find the data *[here](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data)*. The data is all numeric, so the heart diseases are ranked on a scale from 0 to 4, where 0 is completely healthy and 4 is an impending heart-attack. 

.

If you check out that link, you will see that it just leads to a bunch of numbers on a screen. I therefore had to scrape the website and then format the data (a decent amount of work given my relative inexperience in the field), which was done with the code below. Of course, before doing even that, we have to import the necessary libraries, which are 

.

library(rvest)
library(stringr)
library(dplyr)
library(rpart)
library(caret)
library(vcdExtra)
library(ipred)
library(neuralnet)
library(magrittr)
.

```{r, include=FALSE}
library(rvest)
library(stringr)
library(dplyr)
library(rpart)
library(caret)
library(vcdExtra)
library(ipred)
library(neuralnet)
library(magrittr)
```

```{r}
#-----------------------------Getting and processing our data----------------------------------------------
# Scrapes the data
raw_data <- read_html("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data") %>%  #reads the the data from this page
  html_nodes("body") %>%     
  html_text()

# Turns the data into a list 
no_n <- gsub("\n", " ,", raw_data)
row_data <- strsplit(no_n, ",")


data <- data.frame(matrix(unlist(row_data), nrow=303, byrow=T))

colnames(data)[1] <- "Age"
colnames(data)[2] <- "Sex"
colnames(data)[3] <- "Chest_Pain"
colnames(data)[4] <- "Blood_Pressure"
colnames(data)[5] <- "Cholestoral"
colnames(data)[6] <- "Blood_Sugar"
colnames(data)[7] <- "ecg"
colnames(data)[8] <- "Max_Heart_Rate"
colnames(data)[9] <- "eia"
colnames(data)[10] <- "Exercise_induced_feelings"
colnames(data)[11] <- "Slope_of_ST"
colnames(data)[12] <- "Fluoroscopy_stain"
colnames(data)[13] <- "Thal"
colnames(data)[14] <- "Diagnosis"

#taking away all non useful cells
data[ data == "?" ] <- NA
data <- na.omit(data)


head(data)
```

.

As you can see, the data comes out nicely in a data frame. Note that I have taken away all cells that contained "?" (there were a couple) as these would have messed up my models later. 

.

The first thing I decided to was to model it using a decision tree, as this is a fairly simple model, but one that deals well with a large number of factors. To do this, we make use of the Caret package. This not only makes implementation easy, but also bootstraps the data 25 times and gives us a very nice output off results.

.

In order to implement the model, we first divide our data in testing and training parts. I have opted for a 75/25 split, but it doesn't really matter as long as you have a split and the training set is in the significant majority. We do this so that we can realistically test our model. If you don't see why this is necessary, think of it like this... imagine trying to teach a child basic maths. You want to teach him addition, so you tell him 2 + 2 = 4, 2 + 3 = 5 and so on. If you then go back and test him by asking him 2 +2, he should (hopefully) say the answer is 4. However, you can't be sure if he has learnt addition, or if he just knows that 2 + 2 = 4. In order to test him, you ask a sum he hasn't seen before, like 2 + 4. If he answers 6, you're doing a good job... anything else, and not so much. 

.

Back to the code. We then train our model with Diagnosis as a function of everything (thats what ~ . means) and instruct it to use the rpart method, which is a decision tree. The next two lines just show us the details and decision tree for the model. 

.

```{r}
#----------------------------------Training our model---------------------------------------------------

# Splitting the data into training and testing sets

index <- sample(1:nrow(data),round(0.75*nrow(data)))

train <- data[index,]
test <- data[-index,]

# We then train our model using the training data set 
data.rpart <- train(Diagnosis ~ ., data = train, method = "rpart")
# We can look at the details of our model here
data.rpart
# We can look at the decision tree here
data.rpart$finalModel

#--------------------------Testing the rpart model and looking at output---------------------------------

predict(data.rpart, test)

confusionMatrix(predict(data.rpart, test), test$Diagnosis)

```

.

Looking at the output of the code, we see that we have 223 samples for the training set, with 13 predictors and 5 classes. This all sounds good. One interesting aspect of the rpart model is that it is able to give us different complexity parameters and their subsequent accuracy is used in the model. The complexity parameter is effectively a measurement of how deep your decision tree is. If this doesn't make sense, I suggest doing some background reading on it. We see which complexity parameter gives us the highest level of accuracy, which can be seen to be worryingly low here. Not to worry however, as we haven't tested the model yet. We can also have a look at the decision tree if we wish to. 

.

We then moving on to testing our model. We use the predict command, which predicts the Diagnosis of the 75 remaining data points using the 13 factors. We can see the output. This is nice to see, but really that useful. What is useful however, is the confusion matrix. This shows us how many false/true positive/negatives were produced by the model on the testing data set, as well as the overall statistics. We can see the sensitivity and specificity for each result, as well as the overall accuracy. Our accuracy is surprisingly low given that we expected the factors to be highly correlated to the output.   

.

A low accuracy value is not the end of the world, for several reasons. The first of these is we have chosen an inappropriate model for the data set. Following this thinking, I tried a different tact and so I ran a logistical regession model. This has two benefits. The first is that it is another model and so we may find it is accurate. However, it is also a rather simple model and as the decision tree failed, I was not hopeful. The main reason I ran it is that this model is able to tell you which factors are strongly correlated, and which are not, to the output. We want to know this because sometimes having extraneous factors in the model makes it less accurate, and so if we are able to cut some unnecessary ones, it should help. 

.

Once again, we divide the data into testing and training sets. A subtlety here is that the logistic model strictly takes numeric arguments, so we have to make sure the dataframe is a numeric one. Onc again we train the model as Diagnosis as a function of everything with the training data set and then we have a look.

.

```{r, echo=TRUE}
#-----------------------------Using a Logistics model--------------------------------------------------

#We first turn the data into a numeric dataframe and get new training and testing sets
data <- as.data.frame(sapply(data, as.numeric))
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]


logistic <- glm(Diagnosis ~ . , data = train)
summary(logistic)
plot(logistic)
```

.

The first thing that we see when we examine the summary is that not all of our factors are important. We can see it in two ways. The technical way is to see which factor has a small (on the order of 10^(-3)) p factor. The slightly easier way is to look at the stars next to the factors... *** means important,   means not. I was happy to see that 8 of our factors seem to have little or no bearing on the model, and so we can take them out.    

.

I was less happy with the plots. As you can see, the model struggled to successfully model the data points. I was not discouraged however, as I had one more trick up my sleeve... the Neural Network. The Neural Network is a more advanced model that is based upon the human brain. It uses things artificial neurons that act like  real neurons to create models that are exceptionally good at tasks that other models are traditionally not good at, like identifying hand-writing. I won't go into how the model works, but if you are interested, you can read up on it *[here](https://en.wikipedia.org/wiki/Artificial_neural_network).*

.

We begin the model by taking the data that we discovered we didn't need using the GLM model. We then have to normalize the data. This is a particular quirk of the neural network, as if it is not done, it may lead to meaninless or incorrect results.  There are different methods of normalization (z-normalization, min-max etc. ), though I went with the use the min-max method and a scale in the interval [0, 1]. We once again split our data and rthen train the model. I have included an image of what the Neural Network looks like if you are curious. As before, we then test our model against our testing data and then have a look at the results.

.

```{r}
#-------------------------Using a neural network---------------------------------------------------------

#We first take away all superfluous data points to simplify our model
data$Age = NULL
data$Sex = NULL
data$Blood_Pressure = NULL
data$Cholestoral = NULL
data$ecg = NULL
data$Blood_Sugar = NULL
data$Max_Heart_Rate = NULL
data$eia = NULL


# Normalize data. 
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)

scaled <- as.data.frame(scale(data, center = TRUE, scale = maxs - mins))

train.scaled <- scaled[index,]
test.scaled <- scaled[-index,]

# We now train the model
names <- names(train.scaled)
names.all <- as.formula(paste("Diagnosis ~", paste(names[!names %in% "Diagnosis"], collapse = " + ")))
nn <- neuralnet(names.all,data=train.scaled,hidden=c(5,3),linear.output=T)
plot(nn)

# We compute our test data with the model  
pr.nn <- compute(nn,test.scaled[ ,1:5])      


# We undo the normalization by scaling it back up and then calculate 
# the Means Squared error
pr.nn_ <- pr.nn$net.result*(max(data$Diagnosis)-min(data$Diagnosis))+min(data$Diagnosis)
test.r <- (test.scaled$Diagnosis)*(max(data$Diagnosis)-min(data$Diagnosis))+min(data$Diagnosis)

Results_of_Neural_Network <- pr.nn_
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test.scaled)
print(MSE.nn)


# We now plot our results
par(mfrow=c(1,2))
plot(test$Diagnosis,Results_of_Neural_Network, col='blue',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='Neural Network',pch=18,col='blue', bty='n')
```

.

The results, as you can see, were not good. The first indicator of this was the Means Squared Error (MSE). The MSE is defined as "the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors or deviationsâ€”that is, the difference between the estimator and what is estimated." In simpler terms, its an average measure of how far wrong each point was from its predicted value. Our MSE is approxiamtely 30% of our total range. In addition to this, the plot is wildly different to what an accurate plot would look like. As the Diagnosis values are discrete, we obviously expect the X values to be 0, 1, 2, 3 or 4, which is what we see. The difference is in the Y values. If the model was accurate, we would see small bundles of points around the co-ordinates (0,0), (1,1), (2,2), (3,3) and (4,4), which would all lie in the straight line y = x. Our actual output shows the Y values to be scattered all over the place, regardless of the X value. Though there is a slight tendency for the Y values to increase as the X ones do, indicating a correaltion, it is extremely slight and definitely not significant enough for us to accept this as an acceptable model. 

.

Despite, or maybe because, all three of my models completely failed to model my dataset, I still feel that I learnt an invaluable lesson. It is that some data sets are simply too tough to fit using the fairly simplistic methods used in this blog post. The real world is complicated and there are clearly a lot more underlying factors and interconnectedness going on in this data set than I had naively assumed at the start of the project. That, or I just picked a really shit data set... who knows?? These sorts of things are as much art as they are science and sometimes art requires a couple of attempts before it becomes something beautiful. 

.

And so, I wasn't able to solve the "Google your symptoms" problem in one afternoon. Doctors 1, Data-Scientists 0. However, I do think that with a better data set and possibly a slightly different method, it would be possible to get pretty close. That however is a project for another blog post, as I am signing off this one. 

.

Till next time, you stay classy San Diego. 








